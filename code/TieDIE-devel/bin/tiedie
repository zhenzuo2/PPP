#!/usr/bin/env python

###
### TieDIE: Tied Diffusion for Network Discovery
###
###	Version: 
###
###		Multiple (>= 3 inputs) Version
###
###	Authors: 
###
###		Evan Paull (evanpaull@gmail.com)
###
###	Requirements:
###
### 	python 3.7.X+
###		numpy 1.7+ (with pre-computed kernels)
###		scipy 0.12+ (for on-the-fly kernel generation)
###
### Minimum Inputs: 
###		
###		- separate source/target input heat files: tab-separated, 3 columns each with <gene> <input heat> <sign (+/-)>
###		- a search pathway in .sif format (geneA <interaction> geneB)
###
### Outputs:
###
###		Creates a directory in the current working directory, and writes all output to that
###		Information and warnings are logged to standard error


import os, sys, dill
from collections import defaultdict
from optparse import OptionParser
parser = OptionParser()
parser.add_option("-k","--kernel",dest="kernel",action="store",type="string",
        default=None,help="Pre-computed heat diffusion kernel in tab-delimited "
        "form. Should have both a header and row labels. The program will "
        "attempt to use scipy to generate a kernel if none is supplied.")
parser.add_option("-n","--network",dest="network",action="store",
        default=None,help=".sif network file for the curated pathway to search."
        " Each line should have format "
        "'<source> <(-a>,-a|,-t>,-t|,-component>)> <target>'")

# optional arguments
parser.add_option("-p","--permute",dest="permute",action="store",default=50,
        type="int",help="Number of random permutations performed for "
        "significance analysis (default 50)")
parser.add_option("--pagerank",dest="pagerank",action="store_true",
        default=False, help="Use Personalized PageRank to Diffuse")
parser.add_option("-o","--output",dest="output",action="store",default="TieDIE",
        help="Name of directory to be created for results for this run "
        "(defaults to TieDIE)")
parser.add_option("-s","--size",dest="size",action="store",default=1.0)
parser.add_option("-c","--consider",dest="consider_top",action="store",default=3)
parser.add_option("--concensus",dest="concensus",action="store_false",default=True)
parser.add_option("--skip_nullModel",dest="skip_null",action="store_true",default=False)

# data specific for patient-specific networks here
parser.add_option("--d_expr",dest="d_expr",action="store",default=None,
        type="string",help="List of significantly differentially expressed "
        "genes, along with log-FC or FC values (i.e. by edgeR for RNA-Seq or "
        "SAM for microarray data. Generated by a sample-dichotomy of interest")
parser.add_option("-m","--min_hub",dest="min_hub",action="store",default=10,
        type="int",help="minimum number of genes in regulon to consider a TF")

# optional inputs here:
(opts, args) = parser.parse_args()

# local imports assume the directory structure from github . 
sys.path.append(os.path.dirname(sys.argv[0])+'/../lib')
from kernel import Kernel
from distance import ProbDistance
from distributions import Dist
from ppr import PPrDiffuser
from permute import NetBalancedPermuter
import tiedie_util
from linkers import *
from tiedie_util import *
from itertools import combinations
from master_reg import ActivityScores
from consensus import *

if opts.kernel is None:
	#sys.stderr.write("Warning: No kernel file supplied, will use SCIPY to compute the matrix exponential, t=0.1...\n")
	from kernel_scipy import SciPYKernel
	from kernel_tensorflow import TensKernel

def getPVAL(setA, setB, network, diffuser, writeToFile=None):
    # TODO add docstring!

	# Instantiate to perform random permutations of the upstream heats, using the topology of the network
	# to correct for node degree
	perObj_setA = NetBalancedPermuter(network, setA)
	perObj_setB = NetBalancedPermuter(network, setB)
	# Perform the number random permutations specified (default 1000)
	permutedHeats_setA = perObj_setA.permute(PERMUTE)
	permutedHeats_setB = perObj_setB.permute(PERMUTE)
	# scores for each of the permutations
	permuted_kldiv = []

	# diffuse original heats, get the KL divergence between vectors
	setA_diffused = diffuser.diffuse(setA, reverse=False)
	setB_diffused = diffuser.diffuse(setB, reverse=True)
	real_kldiv = ProbDistance.getSymmetricMeasure(setA_diffused, setB_diffused)

	# permuted upstream 'sources'
	for heats in permutedHeats_setA:
		# diffuse the permuted scores, then find the Relevance score for that permuted set 
		diffused_heats =  diffuser.diffuse(heats)
		# get the angle between these
		kldiv = ProbDistance.getSymmetricMeasure(setB_diffused, diffused_heats)
		permuted_kldiv.append(kldiv)
	# permuted downstream 'targets'
	for heats in permutedHeats_setB:
		# diffuse the permuted scores, then find the Relevance score for that permuted set 
		diffused_heats =  diffuser.diffuse(heats)
		# get the angle between these
		kldiv = ProbDistance.getSymmetricMeasure(setA_diffused, diffused_heats)
		permuted_kldiv.append(kldiv)

	# just calculate the number of permuted sets that scored better than the real input set. 
	no_lte = 0.0
	for val in sorted(permuted_kldiv):
		if val <= real_kldiv:
			no_lte += 1
		else:
			break

	if writeToFile:
		fh = open(writeToFile, 'w')
		fh.write("Real:"+str(real_kldiv)+"\n")
		fh.write("Score\n")
		for val in permuted_kldiv:
			fh.write(str(val)+"\n")
		fh.close()

	# fit to lognormal, get p-value
	empirical_pval = (no_lte+1)/(PERMUTE*2+1)
	gauss_fit_pval = Dist.fitLogNorm(permuted_kldiv, real_kldiv)

	return (empirical_pval, gauss_fit_pval)

PERMUTE = int(opts.permute)

# parse network file: use for input validation if heat nodes are not in network
sys.stderr.write("Parsing Network File..\n")
network = parseNet(opts.network)
network_nodes = getNetworkNodes(network)

input_heats = {}
input_actions = {}
input_nodes = set()
for file in args:
	input_heat, signs = parseHeats(file)
	input_heats[file] = input_heat
	input_actions[file] = signs
	for gene in input_heats[file]:
		input_nodes.add(gene)

output_folder = opts.output
if opts.pagerank:
	output_folder += "_PAGERANK"

if not os.path.exists(output_folder):
	os.mkdir(output_folder)

#
# Diffusion Step:
#	Load the heat diffusion kernel and perform a kernel-multiply, or alternatively use supplied
# 	page-rank diffused vectors
#

if opts.pagerank:
	# use PageRank to diffuse heats: create a diffuser object to perform this step
	diffuser = PPrDiffuser(network)
else:
	if opts.kernel is not None:
		sys.stderr.write("Loading Heat Diffusion Kernel..\n")
		# load a heat diffusion kernel to perform diffusion
		if opts.kernel.endswith('.obj'):
			diffuser = dill.load(open(opts.kernel, 'rb'))
		else:
			diffuser = Kernel(opts.kernel)
	else:
		sys.stderr.write("Using SCIPY to compute the matrix exponential, t=0.1...\n")
		# No kernel supplied: use SCIPY to generate a kernel on the fly, and then use it
		# for subsequent diffusion operations
		#diffuser = TensKernel(opts.network)
		diffuser = TensKernel(opts.network)

# normalize input heats to sum to 1
for input in input_heats:
	input_heats[input], signs = normalizeHeats(input_heats[input])

print ("Diffusing Heats...")
diffused_heats = {}
for file in input_heats:
	diffused_heats[file] = diffuser.diffuse(input_heats[file])

tfs_heats = None
# Run Master Regulator analysis on the differential expression list, if
# supplied. Find master regulators and add them to the input set list. Save the
# list of master-regulators in a file and continue with the analysis. 
if opts.d_expr:
	tfs_heats = ActivityScores.findRegulators(network, opts.d_expr, min_hub=opts.min_hub)

	# output inferred tf heats:
	fh = open(output_folder+"/tfs.inferred", 'w')
	for (g, h) in tfs_heats.items():
		fh.write(g+"\t"+str(h)+"\n")
	fh.close()

	##
	## Add inferred heats to the input sets (and diffused sets), which are global variables
	##
	input_heats["tfs"], signs = normalizeHeats(tfs_heats)
	# diffuse heats from these tfs
	diffused_heats["tfs"] = diffuser.diffuse(tfs_heats)

# compute all pairwise distances, validate the input sets first
if not opts.skip_null:
	print ("Computing pairwise p-values between sets")
	print ("\t".join(["Input A", "Input B", "Fitted Pval", "Empirical Pval"]))
	for input1, input2 in combinations(input_heats, 2):

		# print kl divergence
		real_kldiv = ProbDistance.getSymmetricMeasure(diffused_heats[input1],
            	diffused_heats[input2])
		#print "\t".join([input1, input2, str(real_kldiv)])
		output_file = output_folder+"/"+input1.replace('/', '.')+":"+input2.replace('/','.')+".dist.txt"
		empirical_p, fitted_p = getPVAL(input_heats[input1],
            input_heats[input2], network, diffuser, output_file)
		print ("\t".join([input1, input2, str(fitted_p), str(empirical_p)]))

	print ("done")

# do a full TieDIE run: save heat values
# write the cytoscape output file
print ("Running TieDIE with Standard Settings...")
std_options = copy.copy(opts)
subnet_soln, subnet_soln_nodes, linker_heats, cutoff = \
        extractSubnetwork(network, input_heats, diffused_heats, opts.size, std_options)
writeNAfile(output_folder+"/heats.NA", linker_heats, "LinkerHeats")
writeHEATS(output_folder+"/heats.tab", linker_heats)
writeNetwork(subnet_soln, output_folder+"/TieDIE.sif")
print ("done")

# Optional: Run TieDIE on the cohort data, over a range of size-options over a range
# of subsampled inputs. Store a distribution of heats for each node and each 
# edge over subsampling, and the inclusion fractions over the range of size
# options. Save this. This part is for Ed to do.
if not opts.concensus:
	sys.exit(0)

# subsample both upstream/downstream inputs	
options = {}
options['subsample_which'] = "u"
options['size'] = [i/2.0 for i in range(1,10)]
options['network_file'] = opts.network
samples = 100
rate = 0.85

# by convention, the first input should be the mutation data
upstream_heats = input_heats[args[0]]

# get statistics--nodes and edges
print ("Generating consensus TieDIE networks...")
cNet = ConsensusNetwork(network, diffuser)
cNet.generate(input_heats, samples, rate, options)
edge_frequencies, node_frequencies, node_heat_distributions = cNet.getStats()

# write out edge/node fractions
edge_fh = open(output_folder+"/edge_frequencies.txt", 'w')
node_fh = open(output_folder+"/node_frequencies.txt", 'w')
node_heats = open(output_folder+"/node_heats.txt", 'w')
for edge in edge_frequencies:
	edge_fh.write("\t".join(edge)+"\t"+str(edge_frequencies[edge])+"\n")
edge_fh.close()
for node in node_frequencies:
	node_fh.write(node+"\t"+str(node_frequencies[node])+"\n")
node_fh.close()
for node in node_heat_distributions:
	node_heats.write(node+"\t"+"\t".join([str(v) for v in node_heat_distributions[node]])+"\n")
node_heats.close()
print ("done")
