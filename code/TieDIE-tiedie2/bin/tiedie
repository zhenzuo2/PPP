#!/usr/bin/env python

###
### TieDIE: Tied Diffusion for Network Discovery
###
###	Version: 
###
###		Multiple (>= 3 inputs) Development Version
###
###	Authors: 
###
###		Evan Paull (epaull@soe.ucsc.edu)
###
###	Requirements:
###
### 	python 2.7.X
###		numpy 1.7+ (with pre-computed kernels)
###		scipy 0.12+ (for on-the-fly kernel generation)
###
### Minimum Inputs: 
###		
###		- separate source/target input heat files: tab-separated, 3 columns each with <gene> <input heat> <sign (+/-)>
###		- a search pathway in .sif format (geneA <interaction> geneB)
###
### Outputs:
###
###		Creates a directory in the current working directory, and writes all output to that
###		Information and warnings are logged to standard error


import os, sys, dill
from collections import defaultdict
from optparse import OptionParser
parser = OptionParser()
parser.add_option("-k","--kernel",dest="kernel",action="store",type="string",
        default=None,help="Pre-computed heat diffusion kernel in tab-delimited "
        "form. Should have both a header and row labels. The program will "
        "attempt to use scipy to generate a kernel if none is supplied.")
parser.add_option("-n","--network",dest="network",action="store",
        default=None,help=".sif network file for the curated pathway to search."
        " Each line should have format "
        "'<source> <(-a>,-a|,-t>,-t|,-component>)> <target>'")

# optional arguments
parser.add_option("-p","--permute",dest="permute",action="store",default=1000,
        type="int",help="Number of random permutations performed for "
        "significance analysis (default 50)")
parser.add_option("--pagerank",dest="pagerank",action="store_true",
        default=False, help="Use Personalized PageRank to Diffuse")
parser.add_option("-o","--output",dest="output",action="store",default="TieDIE",
        help="Name of directory to be created for results for this run "
        "(defaults to TieDIE)")
parser.add_option("-s","--size",dest="size",action="store",default=1.0)
parser.add_option("-c","--consider",dest="consider_top",action="store",default=3)
parser.add_option("--concensus",dest="concensus",action="store_false",default=True)

# data specific for patient-specific networks here
parser.add_option("--d_expr",dest="d_expr",action="store",default=None,
        type="string",help="List of significantly differentially expressed "
        "genes, along with log-FC or FC values (i.e. by edgeR for RNA-Seq or "
        "SAM for microarray data. Generated by a sample-dichotomy of interest")
parser.add_option("-m","--min_hub",dest="min_hub",action="store",default=10,
        type="int",help="minimum number of genes in regulon to consider a TF")

# optional inputs here:
(opts, args) = parser.parse_args()

# local imports assume the directory structure from github . 
sys.path.append(os.path.dirname(sys.argv[0])+'/../lib')
from kernel import Kernel
from distance import ProbDistance
from distributions import Dist
from ppr import PPrDiffuser
from permute import NetBalancedPermuter
import tiedie_util
from linkers import *
from scoring import Scoring
from tiedie_util import *
from itertools import combinations
from master_reg import ActivityScores
from consensus import *

if opts.kernel is None:
	sys.stderr.write("Warning: No kernel file supplied, will use SCIPY to compute the matrix exponential, t=0.1...\n")
	from kernel_scipy import SciPYKernel

def getUnifiedBackground(sets, network, diffuser, size_control):
	"""
	"""

	# Instantiate to perform random permutations of the upstream heats, using the topology of the network
	# to correct for node degree
	perObj = NetBalancedPermuter(network)
	permuted_input_heats = {}
	diffused_inputs = {}
	for set in sets:
		perObj.setSeedNodes(sets[set])
		# store an array of all permutations of the input heats
		permuted_input_heats[set] = perObj.permute(int(math.ceil(PERMUTE/2.0)))
		diffused_inputs[set] = diffuser.diffuse(sets[set], reverse=False)

	# get the real subnetwork, compute the TieDIE score of the original input sets
	# use the same alpha-cutoff found in the multi-tiedie solution
	linkers, real_score, linker_scores, cutoff = Scoring.scoreInputs(network, sets, diffused_inputs, size_control)

	permuted_scores = []
	for set in sets:
		# hold the other inputs fixed while we permute this one
		# diffuse the permuted scores, then find the Relevance score for that permuted set 
		diffused_permuted = diffuser.diffuse(permuted_input_heats[set])

		# iterate through each permutation of this input set
		for permuted_heats in permuted_input_heats[set]:
			p_input_sets = {}
			p_diffused_input_sets = {}
			for alt_set in sets:
				if alt_set == set:
					# use permutedd scores for this set and diffuse them
					p_input_sets[alt_set] = permuted_heats
					p_diffused_input_sets[alt_set] = diffuser.diffuse(permuted_heats)
				else:
					# all other sets are held to the original values
					p_input_sets[alt_set] = sets[alt_set]
					p_diffused_input_sets[alt_set] = diffused_inputs[alt_set]

			# get the angle between these
			p_linkers, p_score, p_linker_scores, p_cutoff = Scoring.scoreInputs(network, p_input_sets, p_diffused_input_sets, size_control)
			permuted_scores.append(p_score)

	return (real_score, permuted_scores, perObj.getSeed())

def getBackground(setA, setB, network, diffuser, size_control):
	"""
	"""

	# Instantiate to perform random permutations of the upstream heats, using the topology of the network
	# to correct for node degree
	perObj = NetBalancedPermuter(network)
	perObj.setSeedNodes(setA)
	permutedHeats_setA = perObj.permute(int(math.ceil(PERMUTE/2.0)))

	setA_diffused = diffuser.diffuse(setA, reverse=False)
	setB_diffused = diffuser.diffuse(setB, reverse=True)

	# get the real subnetwork, compute the TieDIE score of the original input sets
	# use the same alpha-cutoff found in the multi-tiedie solution
	linkers, real_score, linker_scores, cutoff = Scoring.scoreInputs(network, {'source':setA, 'targets':setB}, {'source':setA_diffused, 'targets':setB_diffused}, size_control)

	# permuted upstream 'sources'
	permuted_scores_fixedB = []
	for permuted_heats in permutedHeats_setA:
		# diffuse the permuted scores, then find the Relevance score for that permuted set 
		diffused_permuted =  diffuser.diffuse(permuted_heats)
		# get the angle between these
		p_linkers, p_score, p_linker_scores, p_cutoff = Scoring.scoreInputs(network, {'source':permuted_heats, 'targets':setB}, {'source':diffused_permuted, 'targets':setB_diffused}, size_control)
		permuted_scores_fixedB.append(p_score)

	return (real_score, permuted_scores_fixedB)

def getPVAL(real_score, bg_scores):
	bg_size = len(bg_scores)

	no_gte = 0.0
	for val in sorted(bg_scores, reverse=True):
		if val >= real_score:
			no_gte += 1
		else:
			break

	empirical_pval = (no_gte+1)/(bg_size+1)

	return empirical_pval

PERMUTE = int(opts.permute)

# parse network file: use for input validation if heat nodes are not in network
sys.stderr.write("Parsing Network File..\n")
network = parseNet(opts.network)
network_nodes = getNetworkNodes(network)

input_heats = {}
input_actions = {}
input_nodes = set()
first = True
for file in args:
	input_heat, signs = parseHeats(file)
	input_heats[file] = input_heat
	input_actions[file] = signs
	for gene in input_heats[file]:
		input_nodes.add(gene)

	weight = 1000
	#if first:
	#	weight = 4000
	#	first = False
	input_heats[file], signs = normalizeHeats(input_heats[file], weight)

output_folder = opts.output
if opts.pagerank:
	output_folder += "_PAGERANK"

if not os.path.exists(output_folder):
	os.mkdir(output_folder)

#
# Diffusion Step:
#	Load the heat diffusion kernel and perform a kernel-multiply, or alternatively use supplied
# 	page-rank diffused vectors
#

if opts.pagerank:
	# use PageRank to diffuse heats: create a diffuser object to perform this step
	diffuser = PPrDiffuser(network)
else:
	if opts.kernel is not None:
		sys.stderr.write("Loading Heat Diffusion Kernel..\n")
		# Dill is the fastest way to load this by far: 
		# Thanks Josh L. Espinoza for adding this option! 
		if opts.kernel.endswith('.obj'):
			diffuser = dill.load(open(opts.kernel, 'rb'))
		else:
		# create the kernel from a tab-separated matrix text file (slow)
			diffuser = Kernel(opts.kernel)
	else:
		sys.stderr.write("Using SCIPY to compute the matrix exponential, t=0.1...\n")
		# No kernel supplied: use SCIPY to generate a kernel on the fly, and then use it
		# for subsequent diffusion operations
		diffuser = SciPYKernel(opts.network)

print "Diffusing Heats..."
diffused_heats = {}
for file in input_heats:
	diffused_heats[file] = diffuser.diffuse(input_heats[file])

tfs_heats = None
# Run Master Regulator analysis on the differential expression list, if
# supplied. Find master regulators and add them to the input set list. Save the
# list of master-regulators in a file and continue with the analysis. 
if opts.d_expr:
	tfs_heats = ActivityScores.findRegulators(network, opts.d_expr, min_hub=opts.min_hub)

	# output inferred tf heats:
	fh = open(output_folder+"/tfs.inferred", 'w')
	for (g, h) in tfs_heats.items():
		fh.write(g+"\t"+str(h)+"\n")
	fh.close()

	##
	## Add inferred heats to the input sets (and diffused sets), which are global variables
	##
	input_heats["tfs"], signs = normalizeHeats(tfs_heats)
	# diffuse heats from these tfs
	diffused_heats["tfs"] = diffuser.diffuse(tfs_heats)

# do a full TieDIE run: save heat values
# write the cytoscape output file
print "Running TieDIE with Standard Settings..."
std_options = copy.copy(opts)
subnet_soln, subnet_soln_nodes, linker_heats, alpha_cutoff = \
        extractSubnetwork(network, input_heats, diffused_heats, opts.size, std_options)
writeNAfile(output_folder+"/heats.NA", linker_heats, "LinkerHeats")
writeHEATS(output_folder+"/heats.tab", linker_heats)
writeNetwork(subnet_soln, output_folder+"/TieDIE.sif")
print "done"

# Optional: Run TieDIE on the cohort data, over a range of size-options over a range

# compute all pairwise distances, validate the input sets first
print "Computing p-value ..."
score, background_scores, random_seed = getUnifiedBackground(input_heats, network, diffuser, opts.size)
empirical_p = getPVAL(score, background_scores)
print empirical_p
random_seed_file = output_folder+'/seed.txt'
fh = open(random_seed_file, 'w')
fh.write(str(random_seed)+'\n')
fh.close()

score_bg = output_folder+'/score.txt'
fh = open(score_bg, 'w')
fh.write('Score'+'\n')
fh.write(str(score)+'\n')
fh.close()

output_bg = output_folder+'/permutations.txt'
fh = open(output_bg, 'w')
fh.write('Score'+'\n')
for bg_score in background_scores:
	fh.write(str(bg_score)+'\n')
fh.close()

print "Computing pairwise p-values between sets"
print "\t".join(["Input A", "Input B", "Fitted Pval", "Empirical Pval"])
for input1, input2 in combinations(input_heats, 2):

	input1_name = input1.split('/')[-1]
	input2_name = input2.split('/')[-1]
	# hold the second input set constant, permute set A and calculate fixed p-value
	scoreA, backgroundA = getBackground(input_heats[input1], input_heats[input2], network, diffuser, opts.size)
	# redo with the other set fixed/variable
	scoreB, backgroundB = getBackground(input_heats[input2], input_heats[input1], network, diffuser, opts.size)
	if scoreA != scoreB:
		print "Error: background scores must be identical!"
		sys.exit(1)
	
	score_bg = output_folder+'/'+input1_name+'_'+input2_name+'.score.txt'
	fh = open(score_bg, 'w')
	fh.write('Score'+'\n')
	fh.write(str(scoreA)+'\n')
	fh.close()

	bg_scores = []
	for score in backgroundA:
		bg_scores.append(score)
	for score in backgroundB:
		bg_scores.append(score)

	empirical_p = getPVAL(scoreA, bg_scores)

	output_bg = output_folder+'/'+input1_name+'_'+input2_name+'.permutations.txt'
	fh = open(output_bg, 'w')
	fh.write('Score'+'\n')
	for bg_score in bg_scores:
		fh.write(str(bg_score)+'\n')
	fh.close()
	print "\t".join([input1, input2, str(empirical_p)])

print "done"				

# Optional: Run TieDIE on the cohort data, over a range of size-options over a range
# of subsampled inputs. Store a distribution of heats for each node and each 
# edge over subsampling, and the inclusion fractions over the range of size
# options. Save this. This part is for Ed to do.
if not opts.concensus:
	sys.exit(0)

# subsample both upstream/downstream inputs	
options = {}
options['subsample_which'] = "u"
options['size'] = [i/2.0 for i in range(1,10)]
options['network_file'] = opts.network
samples = 100
rate = 0.85

# by convention, the first input should be the mutation data
upstream_heats = input_heats[args[0]]

# get statistics--nodes and edges
print "Generating consensus TieDIE networks..."
cNet = ConsensusNetwork(network, diffuser)
cNet.generate(input_heats, samples, rate, options)
edge_frequencies, node_frequencies, node_heat_distributions = cNet.getStats()

# write out edge/node fractions
edge_fh = open(output_folder+"/edge_frequencies.txt", 'w')
node_fh = open(output_folder+"/node_frequencies.txt", 'w')
node_heats = open(output_folder+"/node_heats.txt", 'w')
for edge in edge_frequencies:
	edge_fh.write("\t".join(edge)+"\t"+str(edge_frequencies[edge])+"\n")
edge_fh.close()
for node in node_frequencies:
	node_fh.write(node+"\t"+str(node_frequencies[node])+"\n")
node_fh.close()
for node in node_heat_distributions:
	node_heats.write(node+"\t"+"\t".join([str(v) for v in node_heat_distributions[node]])+"\n")
node_heats.close()
print "done"
